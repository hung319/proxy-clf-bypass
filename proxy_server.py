import os
import logging
from contextlib import asynccontextmanager
from typing import Optional

import cloudscraper
from fastapi import FastAPI, Request, Response as FastAPIResponse, Header, Query, HTTPException
from pydantic_settings import BaseSettings

# --- C·∫•u h√¨nh t·∫≠p trung b·∫±ng Pydantic ---
class Settings(BaseSettings):
    # C√†i ƒë·∫∑t ·ª©ng d·ª•ng
    app_port: int = 5000
    dev_mode: bool = False
    proxy_verbose_logging: bool = False
    expected_api_key: Optional[str] = None
    
    # C√†i ƒë·∫∑t SOCKS5 Proxy
    socks5_proxy_host: Optional[str] = None
    socks5_proxy_port: Optional[int] = None
    socks5_username: Optional[str] = None
    socks5_password: Optional[str] = None

    class Config:
        # T·ª± ƒë·ªông ƒë·ªçc bi·∫øn m√¥i tr∆∞·ªùng, kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# Kh·ªüi t·∫°o settings
settings = Settings()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO if not settings.proxy_verbose_logging else logging.DEBUG)
logger = logging.getLogger(__name__)

# --- Qu·∫£n l√Ω v√≤ng ƒë·ªùi ·ª©ng d·ª•ng v·ªõi Lifespan ---
# T·∫°o m·ªôt context manager ƒë·ªÉ kh·ªüi t·∫°o v√† gi·∫£i ph√≥ng t√†i nguy√™n
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Kh·ªüi ƒë·ªông: T·∫°o m·ªôt scraper instance duy nh·∫•t
    logger.info("Creating a reusable cloudscraper instance...")
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
    )
    
    # C·∫•u h√¨nh proxy cho scraper n·∫øu c√≥
    if settings.socks5_proxy_host and settings.socks5_proxy_port:
        auth = f"{settings.socks5_username}:{settings.socks5_password}@" if settings.socks5_username and settings.socks5_password else ""
        proxy_url = f"socks5h://{auth}{settings.socks5_proxy_host}:{settings.socks5_proxy_port}"
        scraper.proxies = {"http": proxy_url, "https": proxy_url}
        logger.info(f"Cloudscraper is configured to use SOCKS5 proxy: {settings.socks5_proxy_host}")
    
    # G√°n scraper v√†o state c·ªßa app ƒë·ªÉ t√°i s·ª≠ d·ª•ng
    app.state.scraper = scraper
    
    yield
    
    # Shutdown: D·ªçn d·∫πp (v√≠ d·ª•: ƒë√≥ng session)
    logger.info("Closing cloudscraper session.")
    app.state.scraper.close()

# Kh·ªüi t·∫°o FastAPI app v·ªõi lifespan
app = FastAPI(
    title="Optimized Cloudscraper Proxy API", 
    version="2.0.0",
    lifespan=lifespan
)

# --- Logic x·ª≠ l√Ω ch√≠nh ---
async def fetch_url_content(
    scraper: cloudscraper.CloudScraper, 
    target_url: str, 
    referer: Optional[str] = None
):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'en-US,en;q=0.9',
        # B·ªè 'Connection': 'close' ƒë·ªÉ t·∫≠n d·ª•ng keep-alive, tƒÉng t·ªëc ƒë·ªô cho c√°c request li√™n ti·∫øp
    }
    if referer:
        headers['Referer'] = referer

    try:
        # FastAPI s·∫Ω t·ª± ƒë·ªông ch·∫°y h√†m ƒë·ªìng b·ªô n√†y trong m·ªôt thread pool
        # m√† kh√¥ng block event loop ch√≠nh, nh·ªù ƒë√≥ v·∫´n x·ª≠ l√Ω ƒë∆∞·ª£c nhi·ªÅu request
        response = scraper.get(target_url, headers=headers, allow_redirects=True, timeout=20)
        response.raise_for_status()  # N√©m l·ªói cho c√°c status code 4xx/5xx

        return response.content, response.headers.get("Content-Type", "application/octet-stream")

    except Exception as e:
        logger.error(f"Error fetching {target_url}: {e}", exc_info=settings.proxy_verbose_logging)
        # N√©m l·∫°i l·ªói ƒë·ªÉ endpoint c√≥ th·ªÉ x·ª≠ l√Ω v√† tr·∫£ v·ªÅ status code ph√π h·ª£p
        raise HTTPException(status_code=502, detail=f"Failed to fetch upstream URL. Error: {e}")

# --- API Endpoint ---
@app.get("/", response_class=FastAPIResponse)
async def proxy_handler(
    request: Request,
    url: str = Query(..., description="URL to proxy"), # D√πng ... ƒë·ªÉ y√™u c·∫ßu tham s·ªë l√† b·∫Øt bu·ªôc
    referer: Optional[str] = Query(None, description="Optional Referer header"),
    x_api_key: Optional[str] = Header(None, alias="X-API-Key")
):
    # X√°c th·ª±c API Key
    if settings.expected_api_key and x_api_key != settings.expected_api_key:
        raise HTTPException(status_code=403, detail="Invalid or missing API Key.")

    # L·∫•y scraper ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn t·ª´ app state
    scraper_instance = request.app.state.scraper
    
    content, content_type = await fetch_url_content(scraper_instance, url, referer)

    return FastAPIResponse(content=content, media_type=content_type)

# --- Local run ---
if __name__ == "__main__":
    import uvicorn
    logger.info(f"üöÄ Starting server in DEV_MODE at http://0.0.0.0:{settings.app_port}")
    uvicorn.run(
        "proxy_server:app", 
        host="0.0.0.0", 
        port=settings.app_port, 
        reload=settings.dev_mode
    )
