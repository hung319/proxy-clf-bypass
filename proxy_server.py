import os
import logging
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any

import cloudscraper
from fastapi import FastAPI, Request, Response as FastAPIResponse, Header, Query, HTTPException
from pydantic import BaseModel
from pydantic_settings import BaseSettings

# --- C·∫•u h√¨nh t·∫≠p trung b·∫±ng Pydantic ---
class Settings(BaseSettings):
    # C√†i ƒë·∫∑t ·ª©ng d·ª•ng
    app_port: int = 5000
    dev_mode: bool = False
    proxy_verbose_logging: bool = False
    expected_api_key: Optional[str] = None
    
    # C√†i ƒë·∫∑t SOCKS5 Proxy
    socks5_proxy_host: Optional[str] = None
    socks5_proxy_port: Optional[int] = None
    socks5_username: Optional[str] = None
    socks5_password: Optional[str] = None

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# Kh·ªüi t·∫°o settings
settings = Settings()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO if not settings.proxy_verbose_logging else logging.DEBUG)
logger = logging.getLogger(__name__)

# --- Qu·∫£n l√Ω v√≤ng ƒë·ªùi ·ª©ng d·ª•ng v·ªõi Lifespan ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("Creating a reusable cloudscraper instance...")
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
    )
    
    if settings.socks5_proxy_host and settings.socks5_proxy_port:
        auth = f"{settings.socks5_username}:{settings.socks5_password}@" if settings.socks5_username and settings.socks5_password else ""
        proxy_url = f"socks5h://{auth}{settings.socks5_proxy_host}:{settings.socks5_proxy_port}"
        scraper.proxies = {"http": proxy_url, "https": proxy_url}
        logger.info(f"Cloudscraper is configured to use SOCKS5 proxy: {settings.socks5_proxy_host}")
    
    app.state.scraper = scraper
    
    yield
    
    logger.info("Closing cloudscraper session.")
    app.state.scraper.close()

# Kh·ªüi t·∫°o FastAPI app v·ªõi lifespan
app = FastAPI(
    title="Enhanced Cloudscraper Proxy API",
    version="2.1.0",
    lifespan=lifespan
)

# --- Model cho response c·ªßa status ---
class StatusResponse(BaseModel):
    status: str
    message: str

# --- API Endpoint ---

@app.get("/status", response_model=StatusResponse, tags=["Server Status"])
async def get_server_status():
    """
    Cung c·∫•p tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa m√°y ch·ªß.
    """
    return {"status": "ok", "message": "Server is up and running!"}

# --- THAY ƒê·ªîI 1: S·ª≠ d·ª•ng api_route ƒë·ªÉ ch·∫•p nh·∫≠n c·∫£ GET v√† POST ---
@app.api_route("/", methods=["GET", "POST"], response_class=FastAPIResponse)
async def proxy_handler(
    request: Request,
    url: str = Query(..., description="URL to proxy"),
    x_api_key: Optional[str] = Header(None, alias="X-API-Key")
):
    # X√°c th·ª±c API Key
    if settings.expected_api_key and x_api_key != settings.expected_api_key:
        raise HTTPException(status_code=403, detail="Invalid or missing API Key.")

    # L·∫•y scraper ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn t·ª´ app state
    scraper_instance = request.app.state.scraper
    
    # --- THAY ƒê·ªîI 2: X·ª≠ l√Ω request body v√† custom headers ---
    # L·∫•y request body n·∫øu ph∆∞∆°ng th·ª©c l√† POST
    request_body = await request.body() if request.method == "POST" else None

    # L·∫•y v√† l·ªçc c√°c headers t·ª´ request g·ªëc ƒë·ªÉ chuy·ªÉn ti·∫øp
    headers_to_forward = {}
    # C√°c header kh√¥ng n√™n chuy·ªÉn ti·∫øp tr·ª±c ti·∫øp
    excluded_headers = [
        "host", "user-agent", "accept-encoding", "connection", 
        "x-api-key", "content-length", "content-type"
    ]
    for name, value in request.headers.items():
        if name.lower() not in excluded_headers:
            headers_to_forward[name] = value

    # L·∫•y content-type t·ª´ header g·ªëc n·∫øu l√† POST request
    if request.method == "POST" and "content-type" in request.headers:
        headers_to_forward["Content-Type"] = request.headers["content-type"]
    
    content, content_type, status_code = await fetch_url_content(
        scraper=scraper_instance,
        method=request.method,
        target_url=url,
        custom_headers=headers_to_forward,
        post_data=request_body
    )
    
    # Tr·∫£ v·ªÅ response v·ªõi status code g·ªëc
    return FastAPIResponse(content=content, media_type=content_type, status_code=status_code)


# --- THAY ƒê·ªîI 3: N√¢ng c·∫•p h√†m x·ª≠ l√Ω ch√≠nh ƒë·ªÉ h·ªó tr·ª£ c√°c ph∆∞∆°ng th·ª©c v√† header kh√°c nhau ---
async def fetch_url_content(
    scraper: cloudscraper.CloudScraper, 
    method: str,
    target_url: str, 
    custom_headers: Dict[str, Any],
    post_data: Optional[bytes] = None
):
    # C√°c header m·∫∑c ƒë·ªãnh, s·∫Ω b·ªã ghi ƒë√® b·ªüi custom_headers n·∫øu tr√πng l·∫∑p
    base_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    
    # G·ªôp header m·∫∑c ƒë·ªãnh v√† header t√πy ch·ªânh
    final_headers = {**base_headers, **custom_headers}
    
    logger.debug(f"Forwarding {method} request to {target_url} with headers: {final_headers}")
    if post_data:
        logger.debug(f"Forwarding POST data: {post_data[:200]}...") # Log m·ªôt ph·∫ßn body

    try:
        # S·ª≠ d·ª•ng scraper.request ƒë·ªÉ c√≥ th·ªÉ g·ªçi b·∫•t k·ª≥ ph∆∞∆°ng th·ª©c n√†o (GET, POST, ...)
        response = scraper.request(
            method,
            target_url, 
            headers=final_headers, 
            data=post_data,
            allow_redirects=True, 
            timeout=20
        )
        response.raise_for_status()

        # Tr·∫£ v·ªÅ c·∫£ status code ƒë·ªÉ proxy c√≥ th·ªÉ tr·∫£ v·ªÅ ch√≠nh x√°c h∆°n
        return (
            response.content, 
            response.headers.get("Content-Type", "application/octet-stream"),
            response.status_code
        )

    except Exception as e:
        logger.error(f"Error fetching {target_url}: {e}", exc_info=settings.proxy_verbose_logging)
        raise HTTPException(status_code=502, detail=f"Failed to fetch upstream URL. Error: {str(e)}")


# --- Local run ---
if __name__ == "__main__":
    import uvicorn
    logger.info(f"üöÄ Starting server in DEV_MODE at http://0.0.0.0:{settings.app_port}")
    uvicorn.run(
        "__main__:app", 
        host="0.0.0.0", 
        port=settings.app_port, 
        reload=settings.dev_mode
    )
