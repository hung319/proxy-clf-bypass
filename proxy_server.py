import os
import logging
from contextlib import asynccontextmanager
from typing import Optional

import cloudscraper
from fastapi import FastAPI, Request, Response as FastAPIResponse, Header, Query, HTTPException
from pydantic import BaseModel
from pydantic_settings import BaseSettings

# --- C·∫•u h√¨nh t·∫≠p trung b·∫±ng Pydantic ---
class Settings(BaseSettings):
    # C√†i ƒë·∫∑t ·ª©ng d·ª•ng
    app_port: int = 5000
    dev_mode: bool = False
    proxy_verbose_logging: bool = False
    expected_api_key: Optional[str] = None
    
    # C√†i ƒë·∫∑t SOCKS5 Proxy
    socks5_proxy_host: Optional[str] = None
    socks5_proxy_port: Optional[int] = None
    socks5_username: Optional[str] = None
    socks5_password: Optional[str] = None

    class Config:
        # T·ª± ƒë·ªông ƒë·ªçc bi·∫øn m√¥i tr∆∞·ªùng, kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# Kh·ªüi t·∫°o settings
settings = Settings()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO if not settings.proxy_verbose_logging else logging.DEBUG)
logger = logging.getLogger(__name__)

# --- Qu·∫£n l√Ω v√≤ng ƒë·ªùi ·ª©ng d·ª•ng v·ªõi Lifespan ---
# T·∫°o m·ªôt context manager ƒë·ªÉ kh·ªüi t·∫°o v√† gi·∫£i ph√≥ng t√†i nguy√™n
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Kh·ªüi ƒë·ªông: T·∫°o m·ªôt scraper instance duy nh·∫•t
    logger.info("Creating a reusable cloudscraper instance...")
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
    )
    
    # C·∫•u h√¨nh proxy cho scraper n·∫øu c√≥
    if settings.socks5_proxy_host and settings.socks5_proxy_port:
        auth = f"{settings.socks5_username}:{settings.socks5_password}@" if settings.socks5_username and settings.socks5_password else ""
        proxy_url = f"socks5h://{auth}{settings.socks5_proxy_host}:{settings.socks5_proxy_port}"
        scraper.proxies = {"http": proxy_url, "https": proxy_url}
        logger.info(f"Cloudscraper is configured to use SOCKS5 proxy: {settings.socks5_proxy_host}")
    
    # G√°n scraper v√†o state c·ªßa app ƒë·ªÉ t√°i s·ª≠ d·ª•ng
    app.state.scraper = scraper
    
    yield
    
    # Shutdown: D·ªçn d·∫πp (v√≠ d·ª•: ƒë√≥ng session)
    logger.info("Closing cloudscraper session.")
    app.state.scraper.close()

# Kh·ªüi t·∫°o FastAPI app v·ªõi lifespan
app = FastAPI(
    title="Optimized Cloudscraper Proxy API", 
    version="2.1.0", # C·∫≠p nh·∫≠t phi√™n b·∫£n v·ªõi thay ƒë·ªïi m·ªõi
    lifespan=lifespan
)

# --- Model cho response c·ªßa status ---
class StatusResponse(BaseModel):
    status: str
    message: str

# --- API Endpoint ---

# Route ƒë·ªÉ ki·ªÉm tra status
@app.get("/status", response_model=StatusResponse, tags=["Server Status"])
async def get_server_status():
    """
    Cung c·∫•p tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa m√°y ch·ªß.
    """
    return {"status": "ok", "message": "Server is up and running!"}

# Route proxy ch√≠nh - ƒê√É C·∫¨P NH·∫¨T
@app.get("/", response_class=FastAPIResponse)
async def proxy_handler(
    request: Request,
    url: str = Query(..., description="URL to proxy"),
    referer: Optional[str] = Query(None, description="Optional Referer header"),
    key: Optional[str] = Query(None, description="API Key for access") # << ƒê√É THAY ƒê·ªîI
):
    # X√°c th·ª±c API Key t·ª´ tham s·ªë URL 'key'
    if settings.expected_api_key:
        if key is None:
            raise HTTPException(status_code=401, detail="API Key is missing from URL query. Please add '&key=YOUR_KEY'.")
        if key != settings.expected_api_key:
            raise HTTPException(status_code=403, detail="Invalid API Key.")

    # L·∫•y scraper ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn t·ª´ app state
    scraper_instance = request.app.state.scraper
    
    content, content_type = await fetch_url_content(scraper_instance, url, referer)

    return FastAPIResponse(content=content, media_type=content_type)


# --- Logic x·ª≠ l√Ω ch√≠nh ---
async def fetch_url_content(
    scraper: cloudscraper.CloudScraper, 
    target_url: str, 
    referer: Optional[str] = None
):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    if referer:
        headers['Referer'] = referer

    try:
        # FastAPI s·∫Ω t·ª± ƒë·ªông ch·∫°y h√†m ƒë·ªìng b·ªô n√†y trong m·ªôt thread pool
        response = scraper.get(target_url, headers=headers, allow_redirects=True, timeout=20)
        response.raise_for_status() # N√©m l·ªói cho c√°c status code 4xx/5xx

        return response.content, response.headers.get("Content-Type", "application/octet-stream")

    except Exception as e:
        logger.error(f"Error fetching {target_url}: {e}", exc_info=settings.proxy_verbose_logging)
        raise HTTPException(status_code=502, detail=f"Failed to fetch upstream URL. Error: {e}")

# --- Local run ---
if __name__ == "__main__":
    import uvicorn
    logger.info(f"üöÄ Starting server in {'DEV_MODE' if settings.dev_mode else 'PROD_MODE'} at http://0.0.0.0:{settings.app_port}")
    uvicorn.run(
        "__main__:app", 
        host="0.0.0.0", 
        port=settings.app_port, 
        reload=settings.dev_mode
    )
